---
title: "Introductory Data Munging in R"
author: "Ken Oung Yong Quan"
date: "23 July 2016"
output:
  knitrBootstrap::bootstrap_document:
    title: "R Intro"
    theme: journal
    highlight: sunburst
    theme.chooser: TRUE
    highlight.chooser: FALSE
---
# Introductory Data Munging in R
## First things first: RStudio

First off, we'll run through the installation of RStudio for those of you who have yet to download and install. You'll be needing R, which you can find here:

[Click here to download R](https://cran.rstudio.com/)

Next, you'll need RStudio, which is a GUI for using R:

[Click here to download RStudio](https://www.rstudio.com/products/rstudio/download2/)

R is a language used mainly for statistical analysis. It is easily extendible through packages that can be downloaded and installed from the R-repository. To access this repository and install pacakges, use the ```install.packages("Package Name")``` function.

For today, you will be needing the data.table package:

```install.packages(data.table)```

Another package that is useful to have is the swirl tutorial package:

```install.packages(swirl)```

This package includes tutorials into the different classes and functions of R, and will be a useful reference for getting started with R. However, we will not be covering this, snce you can slowly explore this in your free time. While useful, going through the Swirl tutorials can be time consuming.

If at any point in time you are confused about a function in R, you can use the command ```?<function>``` to find out more about the function. For example:

```?install.packages```

## Importing Data
We'll be using a DMEF dataset, which is mostly online shopping data (some phone and mail transactions too). Let's import the data first. If you're using RStudio, both of these steps can be performed using the GUI, and the code will be generated for you.

The read.csv function takes in a csv file and imports it into R. If the file is in your working directory, you will only need the name of the file. If you are trying to import the file from another folder, as shown below, you will need to specify the file path.

The '<-' operator is used to store data in a variable. 
```{r}
# Navigating to your working directory (GUI)
# setwd("C:/Users/Ken/Dropbox/Academics/2016/Sem2/R Intro/bza-r-intro/code")
setwd("~/R Intro/bza-r-intro/code")

# Loading the file (GUI)
#dmef_dataset <- read.csv("C:/Users/Ken/Dropbox/Academics/2016/Sem2/R #Intro/bza-r-intro/data/dmef_dataset.csv")
dmef_dataset <- read.csv("~/R Intro/dmef_dataset.csv")
```


So the code above loads the csv data and saved it to the 'dmef_dataset' variable.

## Getting A Feel
You can take a look at the entire dataset using the View command, ```View(dmef_dataset)```, get a simple summary using the summary command, ```summary(dmef_dataset)```, or see the types of data being stored using the str command, ```str(dmef_dataset)```. 

Let's just use the summary command to see the descriptive statistics for this dataset.

```{r}
summary(dmef_dataset)
```

## Before we go on...

The data that you have currently imported is in a data.frame format. This is the most primitive and basic class for storing data in R. To check this, you can use the class function:

```{r}
class(dmef_dataset)
```


Personally, I like using the **data.table** library. This is an extension of the data.frame class and allows you to manipulate the dataset more easily. You have other options like the standard in-built **data.frame** object, or **dplyr** which works better with ggplot2, but data.table is the one we'll be working with today.

```{r}
# Firstly, let's load the data.table package that you installed earlier.
library(data.table)

# Making the dataframe a datatable
dmef_dataset <- as.data.table(dmef_dataset)
```

Now, you can check the class of the dataset:

```{r}
class(dmef_dataset)
```

As you can see, the data set now has the class "data.table", but it is still a data frame. This is because data table is only an extension of the data frame class. We won't go into too much specifics at this point, so let's continue.

## WTF so many columns

If you've taken a look at the full data set, you might think it's a little large to handle. So let's clean up and filter the data. This is where the data table comes in useful. Let's run through some of the basic functions of a data table that might help.

The data table function takes the general form of data.table(x,y,z), where x denotes the subset of rows, y denotes the subset of columns, and z denotes the grouping criteria for the resultant column. Let's say we want to look at rows 3 to 5

### Sifting through rows

```{r}
dmef_dataset[3:5,]
```

But that doesn't really help with data analysis, since you won't know which rows you are interested in, and you definitely do not want to search through all those rows. Luckily, R can also do that for you. Data tables allow for sorting of values based on the row. Let's try that now. For this, we can for example find the rows that contain a certain product...

```{r}
# To keep it short, let's use the head function to look at the first five entries.
head(dmef_dataset[PRODUCT_NO %in% 987668])
```

Or maybe we want to look at a few products at a time. To do this, we will create a vector of the products we want using the ```c()``` function.

```{r}
head(dmef_dataset[PRODUCT_NO %in% c(987668, 989457)])
```

The first variable in the data.table function can take in any argument. We can include filters to check for specific values, as we have done above, but we can also filter out values outside of a certain range. For example:

```{r}
head(dmef_dataset[EXT_PRICE < 30])
```

We can even throw all of the above together!

```{r}
head(dmef_dataset[EXT_PRICE<70 & PRODUCT_NO %in% c(987669, 989457)])
```

### Sorting columns

data.tables also allow you to filter out columns using their headers. For example, let's take a look at some products and their cost and price.

```{r}
ex1 <- dmef_dataset[,.(PRODUCT_NO, QUANTITY, EXT_PRICE)]
```

You can also generate new columns ased on data from existing columns, similar to what you can normally do in excel, but this time with one line of code. The ```EXT_PRICE``` column takes into account the total price of all the products. So lets try to find the the unit price of each product.

```{r}
#We start by removing duplicate products. The following code removes duplicates from the dataset using the product numbers as a key.

unique_ex1 <- unique(ex1, by = "PRODUCT_NO")

#Next, we'll create a new column called unit_price, and make it equal to the value of EXT_PRICE divided by QUANTITY.

unique_ex1 <- unique_ex1[, unit_price := EXT_PRICE/QUANTITY]

#To better see how the code works, let's take a look at the products with quantity greater than 1

head(unique_ex1[QUANTITY > 1])

```

###Sorting by groups

Lastly, let's look at how to sort the data by groups. For example, let's say we want to find out the sum of the prices for each product. Fo this, we designate the product numbers as the groupings. The code should look something like this:

```{r}
head(ex1[,.(total = sum(EXT_PRICE)), by = PRODUCT_NO])
```

As some of you may have realized, we could have used this to find the unit price of each product much more easily. Let's try and compare the results.

```{r}
head(ex1[,.(unit = sum(EXT_PRICE)/sum(QUANTITY)), by = PRODUCT_NO])
head(unique_ex1)
```

Looks like there's a bit of discrepancy. While the reason may be obvious based on the different ways we approached this, let's try and confirm it with some code:

```{r}
ex1[PRODUCT_NO == 987668, .N, by = EXT_PRICE/QUANTITY]
```

The above code sorted product number 987668 into different categories based on the unit price calculated from each individual transacion. As you can see, there are 69 transactions at 79.95 each, 2 transactions at 59.95 each, and one poor soul who bought the product for 89.95 each. For our first method, we removed any duplicates before calculating anything, and the duplicates were calculated based on product number. Hence, the transactions with different prices were removed, and not counted in calculating the unit price. However, when we calculated it by grouping the product numbers and calculating the overall average, these different prices were taken into account.

The important lesson here is to ensure that whichever method you use to extract data from a dataset, you should always besure that you are getting the correct values. It always helps to find out alternative methods of extracting the same data, just to check if they correlate.

### Moving on, finally

Now that you know the basic functions for managing data in a data.table, let's carry on with our analysis. We'll start by removing the columns we don't care about. The ```dmef_small``` dataset we get will be used for the rest of this exercise.

```{r}
dmef_small <- dmef_dataset[, .(ORDER_NO, ZIP, ORDER_LINE, PRODUCT_CATEGORY_ID, 
                               CHANNEL, PRODUCT_NO, EXT_PRICE, QUANTITY, 
                               RETN_QTY, RETN_REVENUE)]
```

## Weird prices?
We can start doing some simple histograms to better understand the distribution of the data.

For example, let's make a histogram for price.

```{r}
# First I create a new col for price. EXT_PRICE = PRICE * QTY so to get price..
dmef_small[, PRICE := EXT_PRICE/QUANTITY]

# You should see a new column called PRICE
head(dmef_small)

# We want to see the distribution of prices for diff objs
# But in this case, each observation is a transaction
# This means there could be super popular objects that mess things up for us
# So first let's just get prices for unique products
object.price <- dmef_small[, .(PRODUCT_NO, PRODUCT_CATEGORY_ID, PRICE)]
```

Remember the mistake we made earlier when using the unique function, in leaving out different price groups for a product? Let's try to resolve that while using the unique function. We can do this by adding another variable to the unique function, so that it searches for rows that are unique both in price and product number.

```{r}
uniq.object.price <- unique(object.price, by = c("PRODUCT_NO", "PRICE"))
```

Now we're left with 8909 unique products with unique prices to look at. Next, let's plot them on a histogram. The histogram function is simply ```hist(data table)```. There are additional parameters you can define, and you can find out more by typing ```?hist()``` into your console. For now, we will be using ```main``` to determine the name of the histogram, and ```xlab``` to label the x-axis.

```{r}
# Plot prices on a histogram
hist(uniq.object.price[, PRICE],
     main = "Histogram of all prices",
     xlab = "Price")
```

Can't really see much...let's filter to those cheaper than 100. Then, to ensure that we can see the prices more easily, we'll use ```break``` to specify for the histogram to show a hundred bars. This way, each bar will represent a dollar across the x-axis. 

```{r}
hist(uniq.object.price[PRICE < 100, PRICE], breaks=100,
     main = "Histogram of Prices below 100",
     xlab = "Price")
```

Looks like the prices bunch around certain values. Let's try to break this down, and look at the the ending number for the price. To do that, we define a function in r, which removes the decimal place, and then find the remainder when divided by 10, giving the ones place digit. After that, we'll add a new column for the ones place, and plot the histogram.

```{r}
LastDigit <- function(x) { (x - x%%1) %% 10 }

# Add a new col ONES.PLACE

uniq.object.price[, ONES.PLACE := LastDigit(PRICE)] 

hist(uniq.object.price[, ONES.PLACE],
     main = "Histogram of Ones Place",
     xlab = "Ones Place")

#Now, let's look at the cents portion. Again, we'll define a function for the cents part.

Cents <- function(x) { round(100 * (x %% 1)) }

#Creating a column for the cents.
uniq.object.price[, CENTS := Cents(PRICE)]

#We'll plot a Histogram with the same paramenters as before
hist(uniq.object.price[, CENTS], breaks=100,
     main = "Histogram of Cents Portion",
     xlab = "Cents")

# Wow okay looks like there's not much variation
# How about the exact numbers?
table(uniq.object.price[, CENTS])
```

## Lol is this even helpful?
I mean, the prices thing sounds interesting and all, but can we derive any cool business insights using R? 

Actually, that last part was just a warm-up. Retail businesses can sometimes have problems with high return rates. Wouldn't it be useful if we could find out more about why products were being returned? Is it an issue with the pricing? Or is it just people in a particular state that are business-hating selfish scum that return anything they buy. Let's find out!

## Return vs Non-return
First let's split the dataset.

```{r}
# Split into two groups
returned <- dmef_small[RETN_QTY > 0]
not.returned <- dmef_small[RETN_QTY == 0]

# Checks to make sure we're right
table(dmef_small[, RETN_QTY==0])

# This should match up with our number of rows
nrow(returned)
nrow(not.returned)

# Yup looks like it worked, and about 4.75% of all goods were returned?
nrow(returned)/nrow(dmef_small) * 100
```

### Price
Since we've been looking at price so much, was there a difference in the prices of products that were returned vs not returned? Remember we did a quick price sorting earlier, under ```unique.object.price```. We'll use the ```returned``` and ```not.returned``` dataset to filter out the products from the 

```{r}
returned.object.price <- uniq.object.price[PRODUCT_NO %in% returned[, PRODUCT_NO]]
not.returned.object.price <- uniq.object.price[PRODUCT_NO %in% not.returned[, PRODUCT_NO]]

# Any difference in price distribution?
boxplot(returned.object.price[, PRICE],not.returned.object.price[, PRICE])

# Well clearly there are too many outliers. Let's ignore the outliers.
boxplot(returned.object.price[, PRICE],not.returned.object.price[, PRICE],
        main = "Product Prices for Returned vs Not Returned Goods",
        names = c("returned", "not returned"),
        outline=F)
```

Well for the prices, it appears that median price is about the same for both, but products that are returned don't go as expensive as those that are returned. You can tell from a lower 75th percentile, and lower max (excluding outliers). Of course, it's also possible that you observe because of a greater variance. There are after all about 2.7x the number of unique products in the not returned list vs the returned list.

### Total Order Size
Instead of just looking at object price, how about the total spent in that transaction (EXT_PRICE)?

```{r}
boxplot(returned[, EXT_PRICE], not.returned[, EXT_PRICE], outline=F,
        main = "Extended Prices for Returned vs Not Returned Goods",
        names = c("Returned", "Not Returned"))
```
Interestingly, after accounting for outliers, it appears that returned goods tend to be in larger orders than non-returned goods. This might indicate that the people who return their goods might be willing to make larger orders in the knowledge that they could return them later on.

### Product Categories

Now, let's take a look at which product categories tend to have the highest rate of returns. Firstly, we'll use a bar plot to show the proportion of returned goods that came from that product category.

```{r}
barplot(returned[, .N, by = PRODUCT_CATEGORY_ID]/ nrow(returned) *100)
```

```{r}
barplot(table(not.returned[, PRODUCT_CATEGORY_ID])/ nrow(not.returned) *100)

# Percentage of good returned in each product category
perc.returned.by.cat <- dmef_small[, .(Percentage.Returned=.SD[RETN_QTY>0, .N]/.N*100), by=PRODUCT_CATEGORY_ID]
barplot(perc.returned.by.cat[, Percentage.Returned], 
        names=perc.returned.by.cat[, PRODUCT_CATEGORY_ID],
        main = "Percentage of orders returned by category")
```


---TODO---

## Appendix

### Stats to brush up on

### Helpful shortcuts in RStudio

### Other functions to check out

---TODO---
